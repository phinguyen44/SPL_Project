---
title: "Analysis and Graphical Representation of Health and  
Labour Force Participation among the Elderly in Europe"
header-includes:
  - \usepackage{graphicx}
  - \usepackage{amsmath}
  - \usepackage{float}
  - \usepackage{hyperref}
  - \usepackage{setspace}
  - \usepackage{booktabs}
  - \onehalfspacing
output: pdf_document
latex_engine: lualatex
geometry: margin = 2.5cm
bibliography: references.bib
---

```{r, eval = TRUE, echo = FALSE, include = FALSE, cache = TRUE}

wd = paste0(Sys.getenv("HOME"), "/Documents/Projects/SPL_Project")
knitr::opts_knit$set(root.dir = wd)

```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```
\pagenumbering{gobble}


\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
            {-2.5ex\@plus -1ex \@minus -.25ex}%
            {1.25ex \@plus .25ex}%
            {\normalfont\normalsize\bfseries}}
\makeatother
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}    

\bigskip

\begin{center}


Humboldt-Universität zu Berlin \linebreak     
School of Business and Economics  \linebreak
Ladislaus von Bortkiewicz Chair of Statistics   \linebreak
\medskip


\includegraphics[width=0.2\textwidth]{HU_Logo_small.png}


\textbf{Statistical Programming Languages} \linebreak
Winter 2017/18

\medskip


Seminar Paper by  \linebreak  

\textbf{Claudia Günther, Phi Nguyen, Julian Winkel}  \linebreak  
576419, 526624, 562959 \linebreak

\medskip

\medskip


Berlin, 2018-03-15 \linebreak

\end{center}

\medskip

\medskip

\newpage

\listoftables

\newpage

\listoffigures

\newpage


\begin{Large}
\textbf{Abbreviations} \linebreak
\end{Large}

\begin{tabular}{ll}
\textbf{XX} & XXXXXXXXXX     \\

\end{tabular}


\newpage

\pagenumbering{arabic}

\tableofcontents

\newpage

\section{Introduction}

- relevance of exploring relationship between health and labour force participation due to changing demographic in Europe
- cite relevant study about ageing 
- few sentences about relevant papers exploring relationship -> use paper from DIW
- very short literature overview on relationship between health and labour force participation
- share data set as rich data set for this purpose: 2 sentences about it
- introduction of journal article 
- our approach: replicate results an enrich analysis
- especially: introduce graphical visualization tools for descriptive statistics -> ease interpretation of variables
- our aim: write code in a way that allows the user to work with easySHARE data set, even when working on different question

\newpage

\section{Data Cleaning and Manipulation}

The original SHARE data set used by Kalwij and Vermeulen (2005), released in spring 2005, contains data on the life circumstances of eligible members from approximately 18,000 households. In order to be eligible for participation in SHARE, at least one household member must have been born in or before 1954. The SHARE survey contains a variety of different health, social, and socioeconomic indicators such as labor force participation or household composition. SHARE contains data from eleven countries: Austria, Belgium, Denmark, France, Germany, Greece, Italy, the Netherlands, Spain, Sweden and Switzerland. This cross-national, multidisciplinary, and longitudinal nature of the SHARE data set provide an abundance of unique and useful opportunities for analysis.

\subsection{Theory and Design}

The easySHARE data set, released in spring 2017, is the data set for which we will be focusing on in this paper. It is a panel data set of 108 variables of more than 108 variables of more than 100,000 individuals covering data from six survey waves carried out between 2004 and 2015. As can be derived from its name, it is designed to be a simplified version of the broader SHARE data set. As a result, the data set includes slightly different observations and does not provide the same level of granularity in variables broader SHARE data set, which we shall show below. Additionally, since we are only concerned with a smaller subset of observations (from the initial wave of data), we will remove observations that are no longer relevant for our study.

The rest of this section describes our process of data transformation, aggregation, and cleaning , not only to match as closely as possible the initial SHARE data set, but also to convert our data sets into a more accessible state for future analyses. 

\subsection{Implementation}

The `read.and.clean()` function is designed to take the raw easySHARE data set file, which is provided in an `.rda` file format, and convert that into a list of data frames. We split the data set into list items because future analyses involve either summary statistics on the entire data set or regressions conducted on a subset of the data set. The function additionally accepts a wave number as an argument. Although we are only concerned with the initial wave for our study, this additional parameter allows us to conduct supplementary panel studies if we choose to do so later. The function only runs specifically with the raw easySHARE data set (`easySHARE_rel6_0_0.rda`), which is the default `dataset` argument. The `dataset` argument may alternatively be specified to point to the directory where that data set is located. Otherwise it is assumed that it is located in the root directory.

We begin by initializing and downloading the packages (if necessary) required for this function. Much of the funciton utilizes the functions in the `tidyverse` family of functions developed by Hadley Wickham, namely  `dplyr` and `magrittr`. The `dplyr` package is designed for accessible and uncomplicated data manipulation, and it works specifically with data frames. The package provides a consistent set of verbs that do a small thing very well, such as aggregation, selection, filtering, or creation of new variables. The `magrittr` package expands upon this further with the pipe operator `%>%`, which enables the chaining of smaller functions to create something more complex. Furthermore, the pipe operator dramatically improves readability by reading chains left to right as opposed to inside-out. This is especially useful when working with nested functions, which can be deconstructed into a series of chained commands.

```{r eval=FALSE}
neededPackages = c("dplyr", "magrittr", "infuser", "countrycode")
allPackages    = c(neededPackages %in% installed.packages()[,"Package"])

    if (!all(allPackages)) {
        missingIDX = which(allPackages == FALSE)
        needed     = neededPackages[missingIDX]
        lapply(needed, install.packages)
    }

invisible(lapply(neededPackages, function(x) suppressPackageStartupMessages(
    library(x, character.only = TRUE))))
```

We continue with basic filtering to select only variables that are pertinent to our study. For the purposes of our work, we are only concerned with the initial wave and adults between the ages of 50 and 64. Then we encode missing values. Although the overall response rate in the SHARE are comparably high, the data set still has numerous missing values. The reason for this is due to the fact that the study was carried out on a cross-national scale, with some national survey institutions deciding not to participate in all survey modules. The reason for the missing values are documented well in the "Guide to easySHARE release 6.0.0" and specifically coded. For example, the numbers -13 and -14 refer to “not asked in this wave” and “not asked in this country”. Since this coding scheme is not useful for the purpose of our analysis, we recode all of the missing values as "NA". 

```{r eval=FALSE}
# Encode missing values according to SHARE data set guidelines
a = c(-2, -3, -4, -7, -9, -12, -13, -14, -15, -16)
b = c("tocheck","implausible", "tocheck", "uncoded", "notApplicable",
      "dontKnow", "notAskedWave", "notAskedCountry", "noInformation",
      "noDropOff")
missing.value.codes = data.frame(a,b)

# Find NA locations and declare them as such
df.decl = apply(dat, 2, function(z) {
    na.loc    = which(z %in% a)
    z[na.loc] = NA
   return(z)
})
```

We finish converting the raw data set into a final data frame by adding readable country names and transforming raw variables into useful numeric or logical variables as described in the paper. We use the `countrycode` package to change the country codes into human-friendly country names. There are minor differences between the data set that Kalwij and Vermeulen (2005) use and the easySHARE data set, which means we must make certain assumptions in our data cleaning process. For example, certain questions in the survey are an aggregated total of a series of questions, such as the types of chronic diseases the subjects suffer from. Whereas the original authors have access to the specific diseases, we are only provided a count of the total number. Therefore we replace certain variables with the closest relative we have available. In other cases, multiple variables are provided in our data set but it is ambiguous which one the original authors use. An interesting example is employment status: our data set contains one variable for type of employment (full-time, part-time, or not working) and one for labor hours (the number of hours worked by the survey respondent). However, some respondents gave answers that ran counter (for example, saying they worked full-time but only working 20 hours a week). The differences in answers can be attributed to varying interpretations of survey responses.

A simplified example of the final script is shown below.

```{r eval=FALSE}
df.out       = df %>%
        dplyr::left_join(country_data, by = c("country_mod" = "iso3n")) %>%
        dplyr::filter(iso3c %in% country_list) %>%
        dplyr::mutate(country       = factor(country.name.en),
                      gender        = factor(ifelse(female, "FEMALE", "MALE")),
                      age           = factor(floor(age)),
                      edu_low       = isced1997_r %in% c(0, 1),
                      edu_high      = isced1997_r %in% c(5, 6),
                      h_chronic     = chronic_mod,
                      h_overweight  = bmi2 == 3,
                      labor_ft      = ep013_mod >= 32,
                      labor_hrs     = floor(ep013_mod)) %>%
        dplyr::select(country, gender,              
                      starts_with("h_"),            
                      starts_with("labor_")) %>%    
        na.omit() %>%                               # remove missing values
        set_rownames(NULL)                          # reset row numbering
```

Our last step involves splitting the data sets, standardizing any numeric variables, converting the data frames into a model matrix, then storing the resultant data frames into a list that is the final object returned by the function. 

The data frames are split into separate country and gender splits, since we will create separate regressions for each of these splits. Then we apply a custom-built standardization function, which finds standardizes all numeric variables and ignores all other variables. Finally, we use a custom-built function that takes advantage of the base `model.matrix()` function to convert all split data frames into a model matrix. This useful function converts all logical vectors into dummies to replicate the process used in the paper. A corollary benefit is that the resultant data frames can be directly used in R's in-built regression functions.

```{r eval=FALSE}
standardize.df = function(df) {
    idx = sapply(df, is.numeric)
    idx = seq(1:length(idx))[idx]
    
    df.reg = df %>%
        mutate_at(.vars = vars(idx),
                  .funs = standardize) %>%
        mutate(labor_participation = !labor_np) %>% 
        dplyr::select(country, gender, age,
                      h_chronic, h_adla, h_obese, h_maxgrip,
                      edu_second, edu_high, children, couple,
                      labor_participation)
        
    return(df.reg)
}
    
splits    = split(df.out, f = list(df.out$country, df.out$gender), 
                  drop = TRUE)
df.reg    = standardize.df(df.out)
df.splits = lapply(splits, standardize.df)

dummify = function(df) {
    df = df %>%
        dplyr::select(-country, -gender)        
    model      = ~ 0 + .                       
    new.df     = model.matrix(model, df)
    new.df     = data.frame(new.df)
    return(new.df)
}

df.splits = lapply(df.splits, dummify)
```

\subsection{Empirical Results}

The validity of our `read.and.clean()` can be verified by comparing the overall results when running the `read.and.clean()` function with the ones presented in the original paper. As the function is called, messages are printed to the console to show the status of the data reading process. Messages additionally show the names of the items in the list so that users can access the specific data set of their choosing.

In the original paper, the authors retain a sample of 12,237 observations, which is slightly smaller than our retained observations of 12,689. However, at the individual country level, some peculiarities in number of observations emerge. Namely, we have almost twice as many observations in France, while having relative sparsity in Austria. This again is likely due to differences in data set and in the data gathering process. We will discuss more details about the summary statistics in future sections. 

# TODO: HOW THE FUCK DO I SOURCE SHIT

```{r echo=TRUE, eval=TRUE}
# datasets = read.and.clean(dataset = "easySHARE_rel6_0_0.rda", wav = 1)
```

Lastly, when an out-of-bounds value for the wave is selected, an error message occurs. Note that only six waves or cross-sections are included in our data set.

```{r echo=FALSE, eval=TRUE}
# datasets = read.and.clean(dataset = "easySHARE_rel6_0_0.rda", wav = 1)
```

\section{Multidisciplinary and crossnational summary statistics}
JULIAN
\subsection{Theory and Design}
\subsection{Implementation}
\subsection{Empirical Results}


\newpage

\section{Crosssectional probit regression}
JULIAN
\subsection{Theory and Design}
\subsection{Implementation}
\subsection{Empirical Results}


\newpage

\section{Wald Test}
CLAUDIA
\subsection{Theory and Design}
\subsection{Implementation}
\subsection{Empirical Results}


\section{Counterfactual exercise}
CLAUDIA
\subsection{Theory and Design}
\subsection{Implementation}
\subsection{Empirical Results}


\newpage

\section{Graphical Representation}

In this section, we want to introduce novel graphical elements not present in the paper to augment the summary statistic tables. We find that the original paper lacked graphics designed either for exploratory data analysis or to enhance the ease of understanding on behalf of the reader. Tables, although useful for organizing and displaying many data points, can be difficult for a reader to quickly notice trends, similarities, or differences. Furthermore, tables provide decent summary statistics but give no visibility into distributions inherent to the data. We believe that a necessary and important enhancement to the paper would be to include graphs that allow readers to view distributions, analyze trends, and compare data points across groups or geographies. The following section presents functions designed to generate reader-friendly graphs to aid in reader comprehension.

\subsection{Theory and Design}


Another commonly used method for representing country-level data graphically is with choropleth maps.

What are cholopleth maps?
-coloring scheme inside defined areas of a map in order to show value levels
-Great for identifying values that are associated with a given geographic location
-great packages in R with shapes files easily downloadable


What are tile grid maps?
-countries are reduced to a uniform shape and size (equal visual weight)
-great since we don't care about the relative population sizes but percentages or averages

However, we opt to using grid-based maps because they offer a few distinct advantages:
Avoid visual imbalances inherent to traditional choropleths
size of a country can distort our perception of the data and give the impression that (for example) Sweden has a more notable effect
Larger countries carry a more visible weight, influencing a reader's perspective on the relative importance of such country
Sparsely populated areas have larger visual emphasis

One major disadvantage is size dysmorphia (each country appears the same size) 
Also Europe is notoriously difficult to break into evenly sized borders or stack on top of one another
Nevertheless the general shape of Europe 
Also no clearly defined 'rule' of what consitutes Europe
Unlike the US
Often used in the US for voting data
Works well when groups are equally sized

However in our case we are working primarily with numeric data / percentages so that's OK
-not count data
-all on same scale


THEN look at distributions
Why? we need to know the spread, not just the average

\subsection{Implementation}

First step: create a grid map using Excel
use dput to get structure (not shown) (reproducible format without having to explicitly call an excel file)

deciding the correct # of data classes is difficult (too many and you create too many classes) (too little and you mix groups together)
strike the right balance between too much detail and not enough detail

generated by creating a 'base' plotting function then adding the right plotting parameters based on the variable selected
using colors designed to be color-blind safe

Can distinguish differences between countries and within countries based on the selection split

\subsection{Empirical Results}

Improvements:
1. Resize size of squares by population

Straight up, the graphical possibilities of this data set are endless
Would love to try more intresting stuff:

Namely:
We have waves, can do panel data analysis
Can show deciles to show inequalities in health outcomes

From the graphs we can't see much, but shows how little we know!

\newpage


\section{Conclusion}





\newpage

\section{References}


<div id="refs"></div>


\newpage

\begin{Large}
\textbf{Declaration of Authorship}
\end{Large}

\bigskip
\bigskip

We hereby confirm that we have authored this Seminar paper independently and without use
of others than the indicated sources. All passages which are literally or in general matter taken
out of publications or other sources are marked as such.
\bigskip

Berlin, 2018-03-15 

\smallskip

Claudia Günther, Phi Nguyen, Julian Winkel
\bigskip

